---
title: 'Predicting the Quality of Ionosphere-Reflected Pulsed Radar Returns Using a Machine Learning Ensemble Model'
author: 'Sean Pompea'
output: 
  pdf_document:
    toc: true
    toc_depth: 4
    extra_dependencies: ["float"]
date: "2023"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      fig.show='hold',
                      fig.pos = "H", out.extra = "", echo=F, output=F)

options(digits = 3)

# Load packages if needed.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(matrixStats) # colSds() function
library(pROC)
library(knitr)
```

```{r}
#| cache = TRUE
# Download data.
dl <- "ionosphere.zip"
if(!file.exists(dl))
  download.file("https://archive.ics.uci.edu/static/public/52/ionosphere.zip", dl)

data_file <- "ionosphere.data"
if(!file.exists(data_file))
  unzip(dl, data_file, exdir=".")
```

```{r}
# Load data.
d <- read.csv('ionosphere.data', header=F)
y <- d[length(d)]
colnames(y) <- c("y")
d <- d[1:34]
```

Introduction
============

Background
----------

The Earth's ionosphere consists of three primary regions, referred to as regions D, E, and F. The ionosphere is a dynamic system that undergoes continual change---for example, the density of electrons increases during the daytime and decreases at night (Guest 2003). Other phenomena include sudden ionospheric disturbances (SIDs) which are instigated by solar flares (Loudet 2013).

```{r, fig.cap="Diagram of the ionosphere (Guest 2003).", fig.align="center", out.width="55%"}
knitr::include_graphics("figures/ionosphere-diagram.png")
```

Certain radio waves of specific frequencies will travel through the ionosphere and out into space, while others, such as AM and shortwave (both known as high-frequency or HF waves), reflect off the ionosphere and travel back to Earth. 

Radar is a technology that generates radio signals and captures the reflected signal returns, making it a technology that scientists and researchers can use to study the dynamics and physics of Earth's upper ionosphere (specifically, E and F regions), including, in particular, its irregularities (Greenwald et al 1985). One such radar is the installation at Goose Bay, Labrador, an HF (8-20 Mhz) pulsed radar installation consisting of an array of 16 log-period antennae which can send pulsed signals to the ionosphere; the return signals are then processed into data which can be studied (Greenwald et al 1985, Walker et al 1987).

Data collection
---------------

The Goose Bay radar sends out a multipulse signal to the ionosphere, and receives a reflected return signal that is then used to calculate the autocorrelation function (ACF). (In practice, the term ACF is more often used to refer to the calculated data than to the function itself.) The method of calculatng ACFs is outside the scope of the current study, but for details, see Sigillito et al 1989 (and also Wollf). Each ACF value is a complex number consisting of a real part and an imaginary part; in practice, these are handled as two separate values. In the case of the Goose Bay installation, the manner of its specific multiphase pattern emitted from the 16 antennae results in a return signal that resolves into a vector of 17 distinct ACF value pairss---which, for practical purposes, become 34 separate values. Each second, the radar can generate 25 sets of these 17 ACF pairs (Sigillito et al 1989). Scientists use these data to study the ionosphere.

Clutter
-------

In the context of radar, the term clutter refers to unusable data due to noise, interference, and backscatter from unknown or incidental objects, e.g., rain or birds (O'Donnell 2008). In the case of ionosphere-reflected radar returns, the primary sources of clutter are due to signals passing on through the ionosphere (instead of reflecting back), cancellation due to reflection from an overabundance of ionospheric structures, interference from other transmitters (Sigillito et al 1989), and self-clutter arising from signals arriving at the same time but originating from distinct pulses (Reimer and Hussey 2015). 

ACF data therefore needs to cleaned up prior to use, by identifying and removing unusable or 'bad' ACF values, leaving the 'good' values for further study.

Description of the data set
---------------------------

This study made use of the publicly-available Ionosophere data set (Sigillito et al 1988), which was generated by the Goose Bay radar installation in Labrador, and is currently archived at the University of California at Irvine's Machine Learning Repository online archive.

The data set consists of 351 rows. This is about the amount of ACF data that the radar would generate in a time period of 45 seconds (though, that is not to imply that the current data set consists of temporally contiguous readings.) There are 34 independent variables (17 pairs of ACF values, which all together represent the output of a single occurrence of the radar signal and return cycle), and one response variable. 

The response variable is a binary variable which is either 'g' or 'b', for 'good' or 'bad' signal returns. 

Current approach
----------------

In the past, the cleaning of ACF data was done by hand. Nowadays, automated methods are used. Previous work by Sigillito et al (1989) demonstrated the use of neural networks for this task.

For the current study, I hypothesized that a machine learning ensemble model consisting of linear regression, k-nearest neighbors (KNN), and random forest would provide accurate predictions of ACF data quality. Random forest (standalone) was also tested for comparison.

Performance
-----------

The ensemble model achieved an F-score of 0.941 with the holdout data set. Standalone random forest  performed better, with an F-score of 0.956. See Outcomes for further details.

Materials and methods
=====================

Data preparation
----------------

```{r}
# Convert outcomes into 1 and 0.
y <- ifelse(y == "g", 1, 0)
```

The response variable was converted to values of 1 and 0. 

```{r}
# Remove any columns having a standard deviation of zero.
zero_cols <- which(colSds(as.matrix(d)) == 0)
d <- d[-zero_cols]
rm(zero_cols)
```

One column had a standard deviation of 0, and was therefore deemed to have no predictive power and was subsequently removed, leaving a total of 33 independent variables.

Model description
-----------------

A machine learning ensemble model was built by first fitting three individual models---linear regression, k-nearest neighbors (KNN), and random forest---and then combining their predictions into a single set of predictions. While the current study is ultimately a classification task, it can be useful to perform intermediate steps using regression, where predicted outcomes can fall anywhere between 0 and 1. In each case, predictions were first generated as continuous outcomes, and then subsequently these values were then standardized to 0 or 1: values above 0.5 were treated as good outcomes (1), while 0.5 and below were treated as unusable outcomes (0), i.e., clutter. For the ensemble model, each prediction was calculated by taking the average of the continuous versions of predictions from the three underlying models, and then similarly standardizing to 0 or 1. Previous work by Irizarry (2019) provides precedence for using this sort of regression-like approach for a classification task; specifically, see section 27.8 of Irizarry 2019, in which they demonstrate the use of linear regression to classify an image of a written digit as either a 2 or a 7. The benefit of this approach is that, when constructing the ensemble, averages can be calculated, rather than resorting to, e.g., a "majority wins" approach.

The linear regression model had no tunable parameters. After tuning via 10-fold cross validation, KNN's `k` parameter was set to 3. Similarly, after tuning via 10-fold cross validation, random forest's `mtry` parameter was set to 10.

Models were developed using the training and validation sets. Afterward, random forest and the ensemble model were evaluated using the holdout set.

Results
=======

Exploratory analysis
--------------------

```{r}
# Calculate percentages of outcomes.
perc_good <- mean(y == 'g')
perc_bad <- mean(y == 'b')
```

36% of the data represent good signals; 64% represent bad/unusable signals (i.e., clutter). 

Since all independent variables in the data set were calculated using the same ACF method (see the Introduction), it's reasonable to think that distinctions amongst predictors, while present, probably won't be meaningful from an exploratory standpoint (insofar as there is likely no semantic distinction amongst the variables).

Two approaches were used to visualize the data in the hopes of gaining insights: heatmap, and principal component analysis (PCA).

### Distance between good and bad ACF values: heatmap

A heatmap (see Figure 2) of the data set provided some indication, though not a strong one, that there were distinctions between good radar signal ACFs and signal clutter.

```{r, fig.cap="Heatmap of ACF data; green indicates good; pink indicates bad.", fig.align='center'}
# Generate heatmap of data.
heatmap(as.matrix(d),
        RowSideColors = ifelse(y == 1, "green", "pink"))
```

### Principal component analysis

```{r}
# Scale and perform PCA.
d_scaled <- scale(d, center=T, scale=T)
pca <- prcomp(d_scaled, scale.=F)
```

PCA (principal component analysis) showed that there was clustering present between good and bad radar returns. The data set was scaled and centered prior to performing PCA. Figures 3-5 show visualizations of the first three combinations of PCs (principal components). (Note that the response variable was recast as numeric in preparation for development of the ensemble model.)

```{r, fig.cap="Plot of principal components 1 and 2.", out.width="70%", fig.align='center'}
# Plot PC1 vs PC2.
data.frame(pca$x[,1:2], rslt=y) |>
   ggplot(aes(PC1, PC2, fill=y)) +
   geom_point(cex=3, pch=21) +
   coord_fixed(ratio=1)
```

```{r, fig.cap="Plot of principal components 2 and 3.", out.width="70%", fig.align='center'}
# Plot PC2 vs PC3.
data.frame(pca$x[,2:3], rslt=y) |>
   ggplot(aes(PC2, PC3, fill=y)) +
   geom_point(cex=3, pch=21) +
   coord_fixed(ratio=1)
```


```{r, fig.cap="Plot of principal components 1 and 3.", out.width="70%", fig.align='center', fig.show='hold'}
# Plot PC1 vs PC3.
data.frame(pca$x[,1:3], rslt=y) |>
  ggplot(aes(PC1, PC3, fill=y)) +
    geom_point(cex=3, pch=21) +
    coord_fixed(ratio=1)
```

PCA showed that about 48% of the variance in the data could be explained by the first three PCs, and 95% of the variance could be explained by the first 23 PCs. (There were 33 PCs total, corresponding to the 33 independent variables.)

```{r, warning=FALSE}
# Display table with data on first three PCs.
first_3_pcs <- as_tibble(
  cbind(c('Standard deviation', 'Proportion of Variance', 'Cumulative Proportion'),
  summary(pca)$importance))[1:4]
colnames(first_3_pcs) <- c("Measure", "PC1", "PC2", "PC3")
knitr::kable(first_3_pcs)
```
Table: Statistics (standard deviation, variance, and cumulative variance) for the first three principal components.

Figure 6 visualizes the proportion of variance by PC.

```{r, fig.cap="Proportion of variance by PC.", fig.align='center', out.width="70%"}
# Proportion of variance plot.
plot(pca$sdev^2/sum(pca$sdev^2), xlab="PC", ylab="Variance proportion", type="b", col="orange")
```

Data splitting
--------------

```{r}
# Split the data into working and holdout sets.
set.seed(17)
holdout_idx <- createDataPartition(y, times = 1, p = 0.25, list=F)
holdout_set <- d[holdout_idx,] 
y_holdout <- y[holdout_idx]
working_set <- d[-holdout_idx,]
y_working <- y[-holdout_idx]
# Further split working into training and validation sets.
validate_idx <- createDataPartition(y_working, times=1, p=0.33, list=F)
validate_set <- working_set[validate_idx,]
y_validate <- y_working[validate_idx]
train_set <- working_set[-validate_idx,]
y_train <- y_working[-validate_idx]
rm(holdout_idx)
rm(validate_idx)
```

The data was split into train, validation, and holdout sets---50%, 25%, and 25%, respectively, as per Hastie et al (2013)---using a randomized sampling technique.

After splitting the data, the proportion of outcomes in the training set roughly matched the original data set prior to splitting: 

```{r, fig.cap="Distribution of the response variable in the training set.", out.width="50%", fig.align='center'}
# Plot proportions of outcomes in the train set.
y_train |> as_tibble() |>
  ggplot(aes(y_train)) +
  geom_bar() +
  scale_x_continuous(breaks=0:1) +
  xlab("ACF Quality (0=Clutter, 1=Good)") +
  ylab("Tally")
```

Modeling
--------

```{r}
# Set up CV config for reuse.
tr_control <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)
```

First, three models were each employed individually one by one (linear regression, KNN, and random forest). Then, they were combined into an ensemble model.

### Linear regression

```{r, warning=FALSE}
# Fit linear regression model and generate predictions.
fit_lr <- train(train_set, y_train, method="lm", trControl = tr_control)
y_hat_lr_cont <- predict(fit_lr, validate_set)
y_hat_lr <- ifelse(y_hat_lr_cont > 0.5, 1, 0)
```

```{r}
# Determine statistics for LR.
cm_lr <- confusionMatrix(as.factor(y_hat_lr), as.factor(y_validate), positive="1")
outcome_lr <- c(cm_lr$overall['Accuracy'], 
                cm_lr$byClass[c('Sensitivity', 'Specificity', "F1")])
```

The first model employed was linear regression. The training data set was fitted with a linear regression model, predictions were generated, and subsequently those predictions were standardized to 0 or 1 (bad or good ACF values). Its performance was evaluated using the validation set. The results were as follows: the LR model achieved an overall accuracy of 0.862, a sensitivity of 0.982 (an indicator of how often the model correctly identified good ACF values as good), a specificity of 0.645 (an indicator of how often the model correctly identified clutter ACF as clutter), and an F-score of 0.902.

```{r}
# Display results for LR.
kable(data.frame(outcome_lr), col.names = c("Results for linear regression (validation set)."))
```
Table: Statistical results for LR on the validation set.

The receiver operating characteristic (ROC) curve for LR predictions produced an area-under-curve (AUC) value of 0.8137. (AUC values closer to 1 are indicative of an accurate model.) Note that in the case of binary outcomes, the associated ROC curve is, visually, not very exciting, but nonetheless provides some information, particularly regarding the AUC value; this is also the case for ROC curves used to describe clinical tests (see Lalkhen and McCluskey 2008 for example plots). (For an interesting discussion on the topic of visualizations of ROC curves for binary outcomes, see Muschelli 2021.)

```{r, fig.cap="ROC curve for LR.", fig.align='center', out.width="60%"}
# ROC curve for LR.
roc_lr <- roc(y_validate, y_hat_lr)
auc_lr <- round(auc(y_validate, y_hat_lr),4)
ggroc(roc_lr,  colour = 'orange', size = 2) +
  ggtitle(paste0('(AUC = ', auc_lr, ')'))
```

### KNN

```{r, warning=FALSE}
# Fit KNN and generate predictions.
set.seed(17)
fit_knn <- train(train_set, y_train, method="knn", 
                 trControl = tr_control,
                 tuneGrid=data.frame(k=seq(3,13)))
best_k <- fit_knn$bestTune |> pull(k)
y_hat_knn_cont <- predict(fit_knn, validate_set)
y_hat_knn <- ifelse(y_hat_knn_cont > 0.5, 1, 0)
# Determine statistics for KNN.
cm_knn <- confusionMatrix(as.factor(y_hat_knn), as.factor(y_validate), positive="1")
outcome_knn <- c(cm_knn$overall['Accuracy'], 
                cm_knn$byClass[c('Sensitivity', 'Specificity', "F1")])
```

The second model employed was k-nearest neighbors (KNN). 10-fold cross validation was used to determine an optimal value of 3 for tuning parameter `k`. The model performed slightly worse overall than LR, with an accuracy of 0.839, and an F-score of 0.885.

```{r}
# Display table of results for KNN.
kable(data.frame(outcome_knn), col.names = c("Results for KNN (validation set)."))
```
Table: Statistical results for KNN (validation set).

The ROC curve for KNN indicated an AUC of 0.7886---a lower AUC than LR.

```{r, fig.cap="ROC curve for KNN.", fig.align='center', out.width="60%"}
# ROC curve for KNN.
roc_knn <- roc(y_validate, y_hat_knn)
auc_knn <- round(auc(y_validate, y_hat_knn),4)
ggroc(roc_knn,  colour = 'orange', linetype=1, size = 2) +
  ggtitle(paste0('(AUC = ', auc_knn, ')'))
```

### Random forest

```{r, warning=FALSE}
# Fit RF and generate predictions.
set.seed(17)
fit_rf <- train(train_set, y_train, method="rf",
                trControl = tr_control,
                tuneGrid=data.frame(mtry=5:11))
best_mtry <- fit_rf$bestTune |> pull(mtry)
y_hat_rf_cont <- predict(fit_rf, validate_set)
y_hat_rf <- ifelse(y_hat_rf_cont > 0.5, 1, 0)
```

```{r}
# Determine statistics for random forest.
cm_rf <- confusionMatrix(as.factor(y_hat_rf), as.factor(y_validate), positive="1")
outcome_rf <- c(cm_rf$overall['Accuracy'], 
                cm_rf$byClass[c('Sensitivity', 'Specificity', "F1")])
```

Similarly to the approach with LR and KNN, a random forest model was trained on the training data set. 10-fold cross validation was used to choose an optimal value of 10 for the `mtry` tuning parameter (a parameter which determines the number of features considered at successive points where decision trees which make up a random forest are being formed). RF performed well with the validation set, with accuracy of 0.908 and F-score of 0.927---better performance than both LR and KNN.

```{r}
# Display results for random forest.
kable(data.frame(outcome_rf), col.names = c("Results for random forest (validation set)."))
```
Table: Statistical results for random forest (validation set).

For random forest, an AUC of 0.907 was obtained for the ROC curve, which is the highest AUC out of the three models.

```{r, fig.cap="ROC curve for random forest.", fig.align='center', out.width="60%"}
# ROC curve for RF.
roc_rf <- roc(y_validate, y_hat_rf)
auc_rf <- round(auc(y_validate, y_hat_rf),4)
ggroc(roc_rf,  colour = 'orange', linetype=1, size = 2) +
  ggtitle(paste0('(AUC = ', auc_rf, ')'))
```

### Ensemble model

```{r}
# Generate ensemble predictions.
y_hat_ensemble_cont <- (y_hat_lr_cont + y_hat_knn_cont + y_hat_rf_cont) / 3
y_hat_ensemble <- ifelse(y_hat_ensemble_cont > 0.5, 1, 0)
```

For the ensemble model, continuous versions (not yet standardized to 0 or 1) of the predicted outcomes from the three individual models (linear regression, KNN, and random forest) were averaged, and each average was then standardized to 0 or 1. Using the validation set, the ensemble achieved accuracy of 0.885, sensitivity of 0.964, specificity of 0.742, and an F-score of 0.915. This would seem to indicate slightly better performance with the ensemble model than with standalone LR or KNN, but not better than standalone RF.

```{r}
# Determine statistics for ensemble.
cm_ens <- confusionMatrix(as.factor(y_hat_ensemble), as.factor(y_validate), positive="1")
outcome_ens <- c(cm_ens$overall['Accuracy'], 
                cm_ens$byClass[c('Sensitivity', 'Specificity', "F1")])
```


```{r}
# Display results for ensemble.
kable(data.frame(outcome_ens), col.names = c("Results for ensemble model (validation set)."))
```
Table: Statistical results for ensemble (validation set).

For the ensemble model, an AUC of 0.8531 was obtained for the ROC curve.

```{r, fig.cap="ROC curve for ensemble.", fig.align='center', out.width="60%"}
# ROC curve for ensemble model.
roc_ens <- roc(y_validate, y_hat_ensemble)
auc_ens <- round(auc(y_validate, y_hat_ensemble),4)
ggroc(roc_ens,  colour = 'orange', linetype=1, size = 2) +
  ggtitle(paste0('(AUC = ', auc_ens, ')'))
```

Testing with the holdout data set
---------------------------------

```{r}
# Ensemble with holdout; need to build out step by step.

y_hat_lr_cont_hout <- predict(fit_lr, holdout_set)
y_hat_lr_hout <- ifelse(y_hat_lr_cont_hout > 0.5, 1, 0)

y_hat_knn_cont_hout <- predict(fit_knn, holdout_set)
y_hat_knn_hout <- ifelse(y_hat_knn_cont_hout > 0.5, 1, 0)

y_hat_rf_cont_hout <- predict(fit_rf, holdout_set)
y_hat_rf_hout <- ifelse(y_hat_rf_cont_hout > 0.5, 1, 0)

cm_rf_final <- confusionMatrix(as.factor(y_hat_rf_hout),
                            as.factor(y_holdout),
                            positive="1")
outcome_rf_final <- c(cm_rf_final$overall['Accuracy'], 
                   cm_rf_final$byClass[c('Sensitivity', 'Specificity', "F1")])

y_hat_ensemble_cont_hout <- (y_hat_lr_cont_hout + y_hat_knn_cont_hout + y_hat_rf_cont_hout) / 3
y_hat_ensemble_hout <- ifelse(y_hat_ensemble_cont_hout > 0.5, 1, 0)

cm_final <- confusionMatrix(as.factor(y_hat_ensemble_hout),
                            as.factor(y_holdout),
                            positive="1")
outcome_final <- c(cm_final$overall['Accuracy'], 
                   cm_final$byClass[c('Sensitivity', 'Specificity', "F1")])
```


At this point, both random forest and the ensemble model were then tested with the holdout set. See the next section (Outcomes) for details and overall performance statistics.

The ROC curve for ensemble, when testing with the holdout set, yielded an AUC of 0.8906. For RF, the AUC was 0.9353 (compared with the validation AUC of 0.907), which tends to point to the RF model's robustness with new data.

```{r, fig.show='hold', fig.cap="ROC curve for ensemble and RF (using holdout set).", fig.align='center', out.width="49%"}
# ROC curve for ensemble model w/ holdout.
roc_ens_h <- roc(y_holdout, y_hat_ensemble_hout)
auc_ens_h <- round(auc(y_holdout, y_hat_ensemble_hout),4)
ggroc(roc_ens_h,  colour = 'orange', linetype=1, size = 2) +
  ggtitle(paste0('Ensemble with holdout data set (AUC = ', auc_ens_h, ')'))

# ROC curve for RF w/ holdout.
roc_rf_h <- roc(y_holdout, y_hat_rf_hout)
auc_rf_h <- round(auc(y_holdout, y_hat_rf_hout),4)
ggroc(roc_rf_h,  colour = 'orange', linetype=1, size = 2) +
  ggtitle(paste0('RF with holdout data set (AUC = ', auc_rf_h, ')'))
```

Outcomes
--------

When applied to the holdout data set, the ensemble model achieved a sensitivity of 1, and specificity of 0.781. Its F-score of 0.941 improved over the F-score that the model achieved with the validation set (0.915).

The random forest, when applied to the holdout set, performed better overall than the ensemble model, with a sensitivity of 0.964, specificity of 0.906, and F-score of 0.956 (versus 0.927 during validation).

Overall, when tested with the holdout set, random forest performed better than the ensemble model.

Table 6 summarizes this study's findings.

```{r}
# Display table of all outcomes.
all_outcomes <- cbind(outcome_lr, outcome_knn, outcome_rf, outcome_ens, 
                      outcome_rf_final, outcome_final)
colnames(all_outcomes) <- c("LR", "KNN", "RF", "Ensemble", "RF_holdout", "Ensemble_holdout")
kable(data.frame(all_outcomes))
```
Table: Summary of outcomes (unless noted otherwise, results are from the validation set).

Discussion
==========

Sigillito et al's work involved using neural networks. They reported that they achieved a sensitivity and specificity, respectively, of 95.9% and 66.7% for a linear perceptron; 98.4% and 63% for a nonlinear perceptron, and 100% and 88.9% for a multilayer feedforward network (MLFN). The random forest in this study had the closest performance to their MLFN. It seems that various approaches (in their study and this one) leave something to be desired when it comes to specificity.

The current study hints at the idea that a single, apt machine learning model may in cases be preferable to ensemble approaches---or that, really, random forest is simply a generally performant model.

Ensemble's performance with respect to specificity is likely due in part to the underlying models' performance in that category; it is also possible that there is a hidden disadvantage to the simple averaging technique that was employed. Further exploration into more robust methods of constructing ensemble models (perhaps, e.g., weighted averages based on the performance of underlying models, etc.) could prove fruitful.

The small size of the training set (87 rows) may have contributed to overfitting---this possibility cannot be ruled out.

Potential future work in this area would likely need to involve using a larger data set, to ward off overfitting. Ribeiro et al (2013) have developed a simulator capable of generating artificial ACFs, so it could be possible to use that simulator (or a similar one) to generate a very large data set that could facilitate improved model fits.

Conclusions
===========

Machine learning is a feasible approach for predicting the quality of radar ACFs. For this study, an ensemble model was constructed, and then both random forest and the ensemble model were tested with a holdout data set. Random forest performed better than the ensemble model. Pathways of refinement in the future might include exploring more sophisticated methods of constructing ensemble models, and training with a larger data set. Random forest serves as a promising method for the current task of classifying radar returns.

References
==========

R.A. Greenwald, B. Baker, R.A. Hutchins, and C. Hanuise. "An HF phased-array radar for studying small-scale structure in the high-latitude ionosphere." 1985. Radio Science, Volume 20, Number 1, Pages 63-79, January-February, 1985.

P. Guest. "HF and Lower Frequency Radiation - The Ionosphere and the Sun." 2003. https://www.met.nps.edu/~psguest/EMEO_online/module3/module_3_2.html. (Academic Web server with no copyright present.)

T. Hastie, R. Tibshirani, J. Friedman. Elements of Statistical Learning, Second Edition. 2013. Springer.

A.G. Lalkhen and A. McCluskey. "Clinical tests: sensitivity and specificity." Continuing Education in Anaesthesia, Critical Care & Pain, Volume 8, Number 6, 2008, pp. 221-223.

L. Loudet. Sudden Ionospheric Disturbances Monitoring Station A118 website. 2013. https://sidstation.loudet.org/.

R. Irizarry. Introduction to Data Science. 2019. LeanPub. https://leanpub.com/datasciencebook.

J. Muschelli. "ROC and AUC with a Binary Predictor: a Potentially Misleading Metric." J Classif. 2020;37(3):696-708. doi:10.1007/s00357-019-09345-1.

R.M. O'Donnell, "Introduction to Radar Systems, Lecture 7: Radar Clutter and Chaff (slides)." 2008. https://www.ll.mit.edu/outreach/radar-introduction-radar-systems-online-course. 

A.J. Ribeiro, P.V. Ponomarenko, J.M. Ruohoniemi, J.B.H. Baker, L.B.N. Clausen, R.A. Greenwald, and S. de Larquier. "A realistic radar data simulator for the Super Dual Auroral Radar Network." Radio Sci., 2013. doi:10.1002/rds.20032. 

V. Sigillito, S. Wing, L. Hutton, K. Baker. Ionosophere data set. 1988. UC Irvine Machine Learning Repository. https://archive.ics.uci.edu/dataset/52/ionosphere. DOI: 10.24432/C5W01B.

V. Sigillito, S. Wing, L. Hutton, K. Baker. "Classification of Radar Returns from the Ionosphere using Neural Networks." Johns Hopkins APL Technical Digest, Volume 10, Number 3, 1989.

A.D. M. Walker, R. A. Greenwald, and K. B. Baker. "Determination of the fluctuation level of ionospheric irregularities from radar backscatter measurements." Radio Science, Volume 22, Number 5, Pages 689-705, September-October, 1987.

C. Wolff. "Radar Basics: Correlation." No publication date. https://www.radartutorial.eu/10.processing/sp54.en.html.
